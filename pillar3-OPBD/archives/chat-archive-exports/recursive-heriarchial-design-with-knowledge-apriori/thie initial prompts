would it be possible to build a prompting architecture that disallows any input except 'priori' input until a stated problem is solved, and the recursively move thorough defined project stages recursively until the project design is complete?

is this a novel way to use ai

how would 'priori' input be generated  

what are you basing this on?

i believe i am implementing the very model i am proposing in using information 'Priori' do you agree.


****
errata

why can't i see the sources you're using

but i can't follow the links you show, such as likelihood of a desired outcome (Source 2.3).

can you generate the list of sources you used in generating this information so they may be properly cited



please provide an apa style citation list for the following sources you've referenced:

Recursive Decomposition: Frameworks like ReCAP (Recursive Context-Aware Reasoning and Planning), ADaPT (As-Needed Decomposition and Planning for complex Tasks), and others (see search results 1.1, 2.1, 2.3). 

Self-Correction: The iterative loop where the LLM uses self-generated criteria to validate its own output is a key feature of Reflexion and other self-improving agent architectures (search result 2.2).

Preventing Context Drift: The primary purpose of this lock is to prevent context drift and premature collapse (search result 1.3).

ReCAP and similar systems (Source 1.5, 3.4) aim to prevent context drift and hierarchical amnesia, where the LLM forgets the high-level goal as it dives into low-level details (Source 1.4).

They use a hierarchical decomposition (Source 3.2), where a main problem is broken down into sub-tasks, and the context is continuously structured and re-injected to keep the high-level plan proximate to the current execution point (Source 1.1).

Self-correction frameworks like Reflexion, Self-Refine, and CRITIC (Source 4.1, 4.4) are designed to improve output quality by using the LLM itself to detect errors and generate revised steps, often through an explicit "critique" module (Source 2.1).

This aligns with the "Reflect and Adjust" component of the system, where the LLM's reasoning system formulates a plan, adapts to feedback, and evaluates its own actions to correct errors (Source 2.2).

All LLM agents rely on a Core/Brain that orchestrates behavior and a Planning Module that adapts the plan (Source 2.1, 2.5). A crucial part of agent design is Context Engineering, which involves curating the exact set of tokens the LLM sees to maximize the likelihood of a desired outcome (Source 2.3).



are these references complete such that the all the authors you have used in this information are properly credited?